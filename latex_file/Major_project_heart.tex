%\documentclass[11pt]{IEEEtran}
\documentclass[11pt]{article}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage[margin=1in]{geometry}
\usepackage[none]{hyphenat}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{\slshape\MakeUppercase{Heart Disease Prediction}}
\fancyhead[R]{\slshape Shiwam Kumar Sahu}
\fancyfoot[C]{\thepage}
\usepackage{hyperref}

\usetikzlibrary{shapes,arrows,positioning}

\usepackage[none]{hyphenat}
\usepackage{float}
\parindent 0ex
\renewcommand{\baselinestretch}{1.5}
\usepackage[nottoc,notlot,notlof]{tocbibind}
%\renewcommand{\headrulewidth}{0pt}
\setlength{\parindent}{4em}
\renewcommand{\baselinestretch}{1.5}
\begin{document}
\begin{titlepage}
\begin{flushright}
 \textsf{Enrollment No.:2020MSTA016}  
\end{flushright}
\begin{center}
\Large{\textbf{M.Sc. Statistics}}\\
\vspace*{0.5cm}
\Large{\textbf{Major Project Report}}\\
%IF YOU WANT TO CREATE A LARGE SPACE B\W THE HEADING AND THE TITLE OF THE PAPER
\vspace{0.3cm}
\begin{center}
\includegraphics[scale=0.35]{logo1.png}

\end{center}

\vfill
\line(1,0){400}\\[1mm]
\huge{\textbf{Heart Disease Prediction}}\\[3mm]
\large{\textbf{\textit{-Using some statistical tools and Machine learning algorithms-}}}\\[1mm]
\line(1,0){400}\\
\begin{flushleft}
\Large{\textbf{Submitted By:-}}\\
\Large{\textbf{Shiwam Kumar Sahu}}\\

\end{flushleft}
\vspace{-2.5cm}
\begin{flushright}
\Large{\textbf{Supervisor:-}}\\
\Large{\textbf{Dr. Arvind Pandey\\
(Associate Professor)}}\\
\end{flushright}
\vfill

\textbf{Department of Statistics\\
School of Mathematics, Statistics and Computational Sciences\\
Central University of Rajasthan, Ajmer - 305817\\
2020-2022}
\end{center}
\end{titlepage}


\tableofcontents
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}
\begin{center}
\begin{Large}
\underline{\textbf{Abstract}}
\end{Large}
\end{center}
\underline{\textbf{Background:}} Heart disease is one of the leading causes of death in worldwide. In recent time, machine learning play a vital role in the medical field. The aim of our work is the proposal of a dimensionality reduction technique with the help of Principal Component Analysis(PCA) and then performing classification algorithm techniques K- Nearest Neighbor (K-NN), Naive Bayes, Decision tree, SVM kernel(RBF), Adaboost(AB), Gradient Boosting(GB), Decision Tree(DT),Random  Forest(RF). In this work, we had used two Heart disease datasets which are available in Kaggle(an open-source where we find and publish datasets). One is from 1998, which contains 14 features, here Random Forest had the highest accuracy and the other one is from 2020, where we applied same model, SVM kernel had the highest accuracy. We hope the proposed system will be helpful and useful for the physician to diagnose heart disease accurately and effectively.\\
\underline{\textbf{Keywords:}}  K-nearest neighbor, Support vector machine (SVM)(RBF), Decision tree, Dimensonality Reduction(Using PCA), Random forest, Enemble methods.\\

\section{\underline{Introduction}}
The epidemic of cardiovascular diseases(CVD) \cite{abc7} in India is advancing rapidly. The state of affairs has been challenged in the recent  by a series of studies, which shows an alarming rise in case of Cardio Vascular Disease mainly in women. This is not only the case of India but it has been shown that heart disease kills one American every 39 seconds. Heart disease kills more women than all forms of cancer combined. Nearly 40 $\%$ of all deaths in New York were due to Cardiovascular Disease in 2008. The Global Burden of Disease(GBD)\cite{abc8} study reported elimated mortality from coronary heart disease(CHD)(most common type of heart disease) in India at 1.6 million in the year 2000. More women than men now die of heart disease. More than half of preventable deaths that are caused by heart disease and stroke happen to people under the age of 65. CVD claims life of more women every year than all forms of cancer combined. WHO revealed that CVD is expected to affect almost 23.6 million people by the year 2030. European Society of Cardiology (ESC) has published a report in which 26.5 million adults were identified having heart disease and 3.8 million were identifed each year. About 50–55$\%$ of heart disease patients die within the initial 1–3 years, and the cost of heart disease treatment is about 4$\%$ of the overall healthcare annual budget.\cite{abc3}\\
The growth in medical data collection presents a new opportunity for physicians to improve patient diagnosis. In recent years, practitioners have increased their usage of computer technologies to improve 
decision-making support. In the health care industry, machine learning is becoming an important solution to aid the diagnosis of patients. Recently, to
solve difficult issues, a range of data mining techniques and machine learning techniques are built.\\
The contribution of the current work is to introduce an intelligent medical decision system for the diagnosis of heart disease based on contemporary machine learning algorithms.
In this work, we are utilizing supervised learning and also some Ensemble techniques. In the disease prediction machine learning(ML) plays a signficant role. In this work, we predicts whether the patient has a particular disease type or not based on an efficient learning technique.\\
Several supervised learning algorithms like k-nearest neighbor (KNN), support vector machine (SVM),mdecision tree (DT), Naive Bayes (NB), and random forest
(RF) and some Ensemble learning like Gradient Boosting, AdaBoost, XGBoost  are used to classify whether the people tested belong to the class of heart disease or healthy people. Although, Principal component analysis(PCA) are used to select essential features and also reduces the dimension from the dataset which help to give more accurate result in less number of time.
The rest of this work is structured as follows:-\\
Section2 - Describes the proposed architecture and methodology\\
Section3 - Experimental results:-\\
\vspace{0cm}
\begin{itemize}
\vspace{-1.3cm}
\item Dataset discription
\vspace{-0.5cm}
\item Experiment setting
\vspace{-0.5cm}
\item Outcomes
\end{itemize}
 Section4 - Conclusion of our work
\section{\underline{The Proposed Methodology of predicting Heart Disease}}

 Figure \ref{fig:fig11} describes the architecture of the proposed system. It is structured into six stages, including data collection, data preprocessing, Dimensonality reduction using Principle component analysis(PCA), data splitting, training models, and evaluating models.\\
 The steps of the proposed approach are explained in
detail as follows:-
\subsection{\underline{Dataset Discription:}}
In this work, we use two heart disease datasets which are available in Kaggle:-\\
1) First heart disease dataset is from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The "target" field refers to the presence of heart disease in the patient. The target
column includes two classes: 1 indicates heart diseases, and 0
indicates non heart disease. The 14 features together with their descriptions and data types are shown 
in Table(see table \ref{tab:data1} )\\
\begin{table}[H]
\caption{Dataset 1 discription}

\vspace{0.3cm}
\centering
\def\arraystretch{0.9}
\begin{tabular}{|c|c|p{12cm}|}

\hline
No. & Features &        Descriptions\\ \hline
1   &    Age       &    Age of patient (years)\\ \hline
2    &   Sex      &     1: male, 0: female\\ \hline
3    &  Chest pain(CP) &   CP types:-

						1:typical angina,
						
                         2 :atypical angina,
                         
                         3 : nonangina pain,
                         
                         4 : asymptomatic\\ \hline
4 	&   RestBP 	&	  Resting blood pressure\\ \hline
5 	&  Chol 		 &     Serum cholesterol in mg/dl\\ \hline
6 	&   FBS 		&	Fasting blood sugar larger 120 mg/dl (1 true)\\ \hline
7 	&   RestECG   &    Resting electrocardiographic result\\ \hline
8 	&  Talach    &      Maximum heart rate accomplished\\ \hline
9 	&  Exang 	&		 Exercise-induce angina (1 yes)\\ \hline
10 	&  Oldpeak 	&	 ST depression induce: exercise relative to rest\\ \hline
11 	&  CA 	      &    Number of major vessels (0–3)\\ \hline
12 	&  Slope 	 &         Slope of peak exercise ST\\ \hline
13 	&  Thal 		&	  No explanation provided, but probably thalassemia\\ \hline
14 	&   Num      &      Diagnosis of cardiac disease:
					   1: yes
	                    0: no\\
\hline                    


\end{tabular}
\label{tab:data1}

\end{table}
2) Second heart disease dataset is from 2020 annual CDC(Central For Disease Control and Prevention) survey data of 400k adults related to their health status. Originally, the dataset come from the CDC and is a major part of the Behavioral Risk Factor Surveillance System (BRFSS), which conducts annual telephone surveys to gather data on the health status of U.S. residents. As the CDC describes: "Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world". The most recent dataset (as of February 15, 2022) includes data from 2020. It consists of 401,958 rows and 279 columns. The vast majority of columns are questions asked to respondents about their health status, such as "Do you have serious difficulty walking or climbing stairs?" or "Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]". The original dataset of nearly 300 variables was reduced to just about 20 variables. The dataset contains 18 variables (9 booleans, 5 strings and 4 decimals)and total number of rows is 319796. The 18 features together with their descriptions and data types are shown in Table(see table \ref{tab:data2} ).\\
\begin{table}[!ht]
    \caption{Dataset 2 description}

    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        No. & Features & Descriptions \\ \hline
        1 & HeartDisease & Yes or No \\ \hline
        2 & BMI & 12.02 - 94.85 \\ \hline
        3 & Smoking & Yes or No \\ \hline
        4 & AlcoholDrinking & Yes or No \\ \hline
        5 & Stroke & Yes or No \\ \hline
        6 & PhysicalHealth & 0-30 \\ \hline
        7 & MentalHealth & 0-30 \\ \hline
        8 & DiffWalking & Yes or No \\ \hline
        9 & Sex & Male or Female \\ \hline
        10 & AgeCategory & 18-24 to above 80 \\ \hline
        11 & Race & American indian,Asian, Black, Hispanic,White,other \\ \hline
        12 & Diabetic & Yes or No \\ \hline
        13 & PhysicalActivity & Yes or No \\ \hline
        14 & GenHealth & Good,Fair,Very Good, Excellent \\ \hline
        15 & SleepTime & 01-24 \\ \hline
        16 & Asthma & Yes or No \\ \hline
        17 & KidneyDisease & Yes or No \\ \hline
        18 & SkinCancer & Yes or No \\ \hline
    \end{tabular}
    \label{tab:data2}
\end{table}
\clearpage
\subsection{\underline{Data Preprocessing}}
The features are scaled to be in the interval $[-3,3]$ with the help of standardization(or Z-score normalization) process. For standardized value(a z-score), using the formula:-
\begin{center}
$Z$=$\displaystyle\frac{X-\mu}{\sigma}$
\end{center}
where the symbols are:-\\
$X$:observations\\
$\mu$: Mean\\
$\sigma$:Standard deviation

\vspace{1cm}
\begin{figure}


\tikzstyle{block}= [rectangle,draw,fill=blue!30,text width=9em,text centered, rounded corners,minimum height=5em]
\tikzstyle{cloud}= [rectangle,draw,fill=green!30,text width=9em,text centered, rounded corners,minimum height=5em]
%\tikzstyle{decision}=[diamond,draw,fill=blue!20,text width=7em, text badly centered, node distance=2.5cm, innersep=0pt]
\tikzstyle{line}=[draw,very thick, color=black!50,-latex']
\centering
\begin{tikzpicture}
\node[cloud](H1){HEART DISEASE DATASET};
\node[block,below of =H1,node distance=3cm](preprocessing){DATA PREPROCESSING};
\node[block,below of =preprocessing,node distance=3cm](feature){PRINCIPLE COMPONENT ANALYSIS};
\node[block,below of =feature,node distance=3cm](split){DATA SPLIT};
\node[cloud,left of =split,node distance=5cm](train){TRAINING DATASET};
\node[cloud,right of =split,node distance=5cm](test){TESTING DATASET};
\node[block,below of =train,node distance=3cm](model){MODEL TRAINING};
\node[block,right of =model,node distance=5cm](prediction){MODEL PREDICTION};
\node[block,right of =prediction,node distance=5cm](evaluate){EVALUATION METHOD};
\node[cloud,below of =evaluate,node distance=3cm](result){RESULT};

\path[line](H1)--(preprocessing);
\path[line](preprocessing)--(feature);
\path[line](feature)--(split);
\path[line](split)--(train);
\path[line](split)--(test);
\path[line](train)--(model);
\path[line](model)--(prediction);
\path[line](prediction)--(evaluate);
\path[line](evaluate)--(result);
\path[line](test)--(evaluate);
\end{tikzpicture}
\caption{Flowchart of proposed methodology}
\label{fig:fig11}
\end{figure}
\subsection{\underline{Dimensionality reduction using Principal Component Analysis(PCA)}}
The extraction of the best
features is a crucial phase because irrelevant features often
affect the classification efficiency of the machine learning
classifier. Principal Component Analysis(PCA) are used for dimensional reduction without loosing the much more information of the given features and also used to select essential features from the dataset.
\subsection{\underline{Data Splitting}}
In this step, both the heart disease dataset is divided into a $80\%$ training set and a $20\%$ as the testing set.
The training set is utilized for training the models, and the
testing set is utilized to evaluate the models.
\subsection{\underline{Training Models}}
In the training dataset we are applying various contemporary classifcation algorithms: Logistic Regression(LR), Support Vector Machine(SVM), Random Forest(RF), Decision Tree(DT), K-nearest neighbous(K-NN), Gradient Boosting(GB), Ada Boost(AB), XGBoost.
\underline{\textbf{1)Logistic Regression}} is equivalent of linear regression for categorical outcome variable. This can be used where predictors can be categorical or continuous. This is a statistical method for evaluating a dataset in which a result
is calculated by one or more independent variables. It is a supervised learning technique
similar to linear regression. It is used typically in cases when structured model is preferred over data driven models for classification tasks. In this model, categorical outcome variable cannot be directly modeled as an linear function of predictors. So, here instead of using outcome variable(Y) in the model, a function of Y, called \textbf{logit} is used. \\
\textbf{logit:} Think about modeling probability value as a linear function of predictors, specifically in a two class case. If P is the probability of class 1 membership,
\begin{center}
  $P =\beta_{0}+\beta_{1}*X_{1}+\beta_{2}*X_{2}+...+\beta_{p}*X_{p}$
\end{center}

where p is the number of predictors.\\
Since, in logistic regression L.H.S range improves from $\{0,1\} $ to $[0,1]$ but R.H.S range is $\{-\infty,\infty\}$. So, typically a non linear function is used to approach to bring L.H.S range equal to R.H.S range that is,
\begin{center}
$P=\displaystyle\frac{1}{1+\exp^{-(\beta_{0}+\beta_{1}*X_{1}+\beta_{2}*X_{2}+...+\beta_{p}*X_{p})}}$
\end{center}
This function is called logistic response function. Now rearrange the previous two equations as below:\\
\begin{center}
$\displaystyle\frac{P}{1-P}=\displaystyle\exp^{\beta_{0}+\beta_{1}*X_{1}+\beta_{2}*X_{2}+...+\beta_{p}*X_{p}}$
\end{center}
where $\displaystyle\frac{P}{1-P}=\textbf{odds}$.\\
Odds of belonging to a class is defined as ratio of probability of class 1 membership to probability of class 0 membership. Now the range of previous equation to be $\{0,\infty\}$. Taking $\log$ in both side,
\begin{center}
$\log(odds)=\beta_{0}+\beta_{1}*X_{1}+\beta_{2}*X_{2}+...+\beta_{p}*X_{p}$
\end{center} 
This is called \textbf{standard logistic model}. Now L.H.S and R.H.S both have range $\{-\infty,\infty\}$. $\log(odds)$ is called logit. It is used as the outcome variable in the model instead of categorical Y. Odds and logit can be written as a function of probability of class 1 membership.\\
\underline{\textbf{2)Decision Tree}} use multiple algorithms to decide to split a node into two or more
sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, it can be said that the purity of the node increases with respect to the target
variable. The decision tree splits the nodes on all available variables and then selects the split which results in the most homogeneous sub-nodes. Every internal node carries a test on features, and
branches carry the test conclusion, and the class label
is meant for each leaf node. It is utilized both for
classifications and regression.\\
Given a database $D = \{t_{1},t_{2},...,t_{n}\}$ , where $t_{i}$ denotes a tuple, which is
defined by a set of attribute $A = \{A_{1}, A_{2},..., A_{m}\}$. Also, given a set of classes
$C = \{c_{1}, c_{2},...,c_{k}\}$.\\
A decision tree T is a tree associated with D that has the following properties:
\begin{itemize}
\item Each internal node is labeled with an attribute $A_{i}$
\item Each edges is labeled with predicate that can be applied to the attribute
associated with the parent node of it
\item Each leaf node is labeled with class $c_{j}$
\end{itemize}
In principle, there are exponentially many decision tree that can be constructed from a given database(also called training data). Some of the tree may not be optimum and some of them may give inaccurate result. There are mainly two approaches are known in DT:
\begin{itemize}
\vspace{-0.3cm}
\item Greedy strategy
\vspace{-0.5cm}
\item Modification of greedy strategy
\vspace{-0.3cm}
\begin{itemize}
\vspace{-0.3cm}
\item ID3
\vspace{-0.3cm}
\item C4.5
\vspace{-0.3cm}
\item CART
\vspace{-0.3cm}

\end{itemize}
\end{itemize}
\underline{\textbf{3)Random Forest}} is considered as a highly accurate and robust method because of
the number of decision trees participating in the process. It tries to build k different decision trees by picking a random subset S of training samples. It generates fully Iterative
Dichotomiser 3 (ID3) trees with no pruning. It makes a final prediction based on the mean of each prediction, and it tends to be robust to overfitting, mainly because it takes the average of all the predictions, which cancels out biases. As the name suggest as forest the random forest classifier is an ensemble of decision trees where a random vector sample produce each classifier from input vector  and each tree cast 
a unit vote for the most popular class to classify an input vector, most of the time trained with a bagging 
method.\\
A random forest is a classifier consisting of a collection of tree structured classifiers $\{h(x,\theta_{k}),k=1,2,3,...\}$ where the $\theta_{k}$ are independently, identically distributed random trees and each tree casts a unit vote for the final classification of input $x$. Like CART(classification and regression trees), RF uses the \textbf{gini index } for determining the final class in each tree. The final class of each tree is aggregated and voted by weighted values to construct the final classifier.\\
\underline{\textbf{Gini Index:}} The gini index is mainly the impurity measures of the node. The gini index of node impurity is the measure most commonly chosen for classification-type problems. If a dataset $T$ contains  examples from $n$ classes
\begin{center}
Gini($T$) is defined as:
$Gini(T)$=1-$\sum\limits_{j=1}^{n}(p_{j})^{2}$
\end{center}
If a datset $T$ is split into two subsets $T1$ and $T2$ with sizes $N1$and $N2$ respsectively, the gini index of the split data contains examples from $n$ classes, the gini index($T$) is defined as:
\begin{center}
$Gini_{split}(T)$=$\displaystyle\frac{N_{1}}{N}$gini($T_{1}$) + $\displaystyle\frac{N_{2}}{N}$gini($T_{2}$)

\end{center}
\underline{\textbf{Working algorithm of Random Forest(RF):}}
\begin{enumerate}[1.]
\item A random seed is chosen which pulls out at random a collection of samples from the training dataset while maintaining the class distribution.
\item With this selected dataset, a random set of attributed from the original dataset is chosen based on user defined values. All the input variables are not considered because of enourmous computation and high chances of overfitting.
\item In a dataset where M is the total number of input attributes in the dataset, onLy R attributes are chosen at random for each tree where R$<$M.
\item The attributes from this set creates the best possible split using the gini index to develop a decision tree model. The process repeats for each of the branches until the termination condition starting that leaves are the nodes that are too small to split.
\end{enumerate}
\underline{\textbf{4)Naive Bayes}} model is very effective for large datasets because of its simplicity. It
works on the probability basis $ p(c | x)$, where $p(c | x)$ is the posterior probability of the
class $(c)$ and predictor $(x)$. This is a probabilistic statistical base classifier based upon Bayes’ theorem which is strong supervised machine learning classification technique. It assumes that all the features are 
conditionally independent which means the effect of an attribute value has no effect on other attribute value. Naïve Bayes is a very light weight classifier can be used to classify big dataset easily. It is very robust to ignore noise and irrelevant attributes. It is very easy to construct and no need 
of complicated iterative parameter estimation schemes.\\
In Naive bayes, all records are used instead of relying on just the matching records.\\
\underline{\textbf{Naive bayes modification:}}
\begin{itemize}
\item For class $i$ of outcome variable, compute the probabilities $(P_{1},P_{2},...,P_{p})$ of belonging to class $i$ for each predictor's value $(X_{1},X_{2},...,X_{p})$ taken by the new observation to be classified.
\item compute $P_{1}*P_{2}*...*P_{p}*P(c_{i})$, here $P_{c_{i}}$ is the proportion of record that are belonging to class $i$.
\item Execute previous two steps for all the classes
\item To compute the probability of the new observation belonging to class $i$, divide the value computed in step 2 by the summation of values computed in step 2 for all the classes
\item Execute previous step for all the classes 
\item Classify the new observation to the class with the highest probability value.  
\end{itemize}
\underline{\textbf{Naive bayes formula:}}\\
$P(C_{i}/X_{1},X_{2},...,X_{p})=\frac{[P(X_{1}/C_{i})*P(X_{2}/C_{i})*...*P(X_{p}/C_{i})]*P(C_{i})}{[P(X_{1}/C_{1})*P(X_{2}/C_{1})*...*P(X_{p}/C_{1})]*P(C_{1})+...+[P(X_{1}/C_{m})*P(X_{2}/C_{m})*...*P(X_{p}/C_{m})]*P(C_{m})}$


Naive bayes formula is directly derived fromt the exact bayes formula after making following assumptions:
\vspace{-0.5cm}
\begin{itemize}
\item Predictor's values $(X_{1},X_{2},...,X_{p})$ occur independent of each other for a givene class.\\
$P(X_{1},X_{2},...,X_{p}/C_{i})=P(X_{1}/C_{i})*P(X_{2}/C_{i})*...*P(X_{p}/C_{i})$
\item For classification, naive bayes formula works quite well
\item Since, we don't require probability values to be accurate in absolute term, rather just a reasonably accurate rank ordering of these values.
\item For the same reason, we should use the numerator only and drop the denominator which is common for all the classes.
\end{itemize}
\underline{\textbf{5)K-Nearest Neighbours(K-NN)}}\\ K-NN is non-parametric method, as it does not consider the dimensionality of dataset for diagnosis because it relies upon nearest training data points. The “GridSearchCv” was used to figure out the total number of neighbors for the KNN training needed to achieve superior performance. In K-NN useful information for modeling is extracted using the similarities between the records based on predictors values, typically disctance based similarity measures are used. Most popular metric is euclidean distance for measuring the distance between two records. Consider two recors having values of the predictors denoted by $(X_{1},X_{2},X_{3},...,X_{p})$ and $(W_{1},W_{2},W_{3},...,W_{p})$ then:
\begin{center}
$D_{eu}$=$\sqrt{(X_{1}-W_{1})^{2}+(X_{2}-W_{2})^{2}+(X_{3}-W_{3})^{2}+...+(X_{p}-W_{p})^{2}}$
\end{center}
Euclidean distance is preferred in K-NN due to many distance computations. The main idea of K-NN is to find K record in the training partition which are neighboring the new observation to be classified. These K neighbors are used to classify the new observation into a predominant class among the neighbors.\\
\underline{\textbf{6)Boosting}} means producing a model sequence that
aims to correct the errors that have arisen in the
models. In boosting based on the previous model’s elements that are not properly classified, new samples
are produced. Then, by combining the weak models,
the ensemble method increases its efficiency.\\
\underline{\textbf{7)SVM(support vector machine)}} Given a set of data with N attributes, Support Vector Machine (SVM) classifier is to
find a suitable hyper plane in N-Dimensional space that clearly classify the dataset with
a maximum margin between data points, where it segregates the two main classes hyper-plane and line to separate the available sets of points, and it is considered a supervised machine learning algorithm which can be used for classification. Support vector machine (SVM) is considered as a supervised machine learning classification 
technique that is built, based on the concept of decision planes that define decision boundaries. This 
algorithm function by making “hyperplane” and categories the data based on class values, SVM 
algorithm performs margin maximization which means it tries to make maximum difference between 
classes . SVM creates complex non-linear boundaries that are robust to over fitting and 
the major advantage is high classification accuracy. There are two two types of SVM one is linear and other one is non linear SVM. On this two heart disease datasets we are applying non linear svm. In non linear SVM kernel trick is introduced.Kernel trick helps to map from a low dimension space to the high dimension space. There is a simple operation on two vectors in the low-D space that can be used to compute the scalar product of their two images in the high-D space.
\begin{center}
$K(x^{a},x^{b})$=$\phi(x^{a})$.$\phi(x^{b})$
\end{center}
\underline{\textbf{Some commonly used kernels:-}}
\begin{enumerate}
\vspace{-0.3cm}
\item Polynomial:$K(x,y)$=$(x.y+1)^{p}$ 
\vspace{-0.4cm}
\item Gaussian radial basis function (R.B.F)=$K(x,y)$=$\displaystyle\exp^{\displaystyle\frac{-||x-y||^{2}}{2\sigma^{2}}}$
\vspace{-0.3cm}
\item Neural net: $K(x,y)$=$\tanh(k.x.y-\delta)$
In this work we have used kernel(RBF) SVM, which gives best resut in the second HDD.
\end{enumerate}


\subsection{\underline{Evaluating Models}}
For evalauation of the proposed model we are focusing on some criteria, Accuracy, Recall, Precision, F-score.
The confusion matrix (see table \ref{tab:data3} ) helps practitioners to form a clear idea of 
whether the results have a high performance.
\begin{table}[H]
\caption{Confusion Matrix}

\centering

    \begin{tabular}{|c|c|c|}
    \hline
        Class & Predicted Class=0 & Predicted Class=1  \\ \hline
        Actual Class=0 & True Negative(TN) & False Positive(FP) \\ \hline
        Actual Class=1 & False Negative(FN) & True Positive(TP)  \\ \hline
      
    \end{tabular}
\label{tab:data3}
\end{table}
The confusion matrix elements were:
\begin{enumerate}
\vspace{-0.3cm}
\item True positive (TP), which were patients who had heart disease and were correctly diagnosed; 
\vspace{-1cm}
\item True negative (TN), which 
were patients who did not have heart disease and were correctly diagnosed; 
\vspace{-0.3cm}
\item False negative (FN), which were patients who had heart disease and were misdiagnosed; and 
\vspace{-0.3cm}
\item False positive (FP), which were 
patients who did not have heart disease and were misdiagnosed. In the 
medical field, false negatives are the most dangerous predictions

\end{enumerate}  
The different performance metrics were calculated using a confusion 
matrix. Accuracy (Acc) measured the properly classified instances. 

\begin{itemize}
\item The formula for calculating accuracy was given by:\\
Accuracy=$\displaystyle\frac{TP+TN}{TP+FN+FP+TN}$
\item Recall identified the proportion of patients with heart disease given 
by:\\
Recall=$\displaystyle\frac{TP}{TP+FN}$
\item Precision was the positive predictive value defined by:\\
Precision=$\displaystyle\frac{TP}{TP+FP}$
\item The F1 score considered a harmonic average between precision in 
and recall defined by:\\
F1 score=2$\left(\displaystyle\frac{Precision* Recall}{Precision + Recall}\right)$ 
\end{itemize}
\section{\underline{Experimental Results}}

In this section of the work discusses the experimental results of various classification algorithms. At first, we performs the feature scaling with the help of standardization(or Z- score) technique in which the values lie between $[-3,3]$. Then we are applying Principle component analysis(PCA) which reduce the dimension of the dataset. Here the below flowchart show that total number of features present in both the dataset reduces to the 10 and 11 features.  \\

\tikzstyle{block}= [rectangle,draw,fill=blue!30,text width=9em,text centered, rounded corners,minimum height=5em]
\tikzstyle{cloud}= [rectangle,draw,fill=green!30,text width=9em,text centered, rounded corners,minimum height=5em]
%\tikzstyle{decision}=[diamond,draw,fill=blue!20,text width=7em, text badly centered, node distance=2.5cm, innersep=0pt]
\tikzstyle{line}=[draw,very thick, color=black!50,-latex']
\begin{center}
\begin{tikzpicture}
\node[cloud](H2){13 Features of first HD dataset};
\node[block,right of =H2,node distance=8cm](H3){Reduces to 10 features};
\path[line](H2)--node[pos=0.5,above][color=red]{After applying PCA}(H3);
\end{tikzpicture}\\
\begin{tikzpicture}
\node[cloud](H4){17 Features of second HD dataset};
\node[block,right of =H4,node distance=8cm](H5){Reduces to 11 features};
\path[line](H2)--node[pos=0.5,above][color=red]{After applying PCA}(H3);
\end{tikzpicture}

\end{center}
The main purpose of applying PCA in the dataset is that, it not only reduces the dimension but it also reduces the time taken by the model to the dataset which are generated by us. Then the performance of all used classifcation models i.e. K-Nearest Neighbors (KNN), Decision Tree(DT), Random Forest (RF), Logistic Regression (LR), Naïve Bayes (NB), Support Vector Machine (SVM), Adaboost (AB), Gradient Boosting (GB), XGBoost along with this reduced feature space is evaluated.\\
Table \ref{tab:data4} shows that the Random Forest(RF) achieved the highest performance for the first dataset with accuracy, recall, and precision, which are 100$\%$, 100$\%$  and  97$\%$ respectively. The worst performance achieved by Logistic Regression with accuracy, recall, and precision, which are 86.34$\%$ 78$\%$  and  94$\%$. Table  \ref{tab:data4} has constructed  as decresing order of accuracy.

\begin{table}[H]
\caption{ Results of different classification algorithms}

\centering
\def\arraystretch{0.9}
\begin{tabular}{|c|c|c|c|}
\hline
Techniques      &    Accuracy(in $\%$) &  Recall(in $\%$)           &   Precision(in $\%$)  \\ \hline
Random forest   &     100			&  100		   &    97\\
XGBoost          &       98.53      &  100          &       97\\
Decision tree  &       98.53			&  100		   &     97\\
AdaBosst          &      96.58      &  95           &      98\\    
Gradient boost    &     96.09       &  95           &      97 \\ 
SVM (RBF)	     &     90.73        &  86			&    95   \\ 
K-NN 		  &         90.24       &   98		   &     85\\
Logistic regression &    86.34		&   78         &      94\\ 
   \hline

\end{tabular}
\label{tab:data4}
\end{table}
Now, after achieving this accuracy, recall and precisions, similar model has applied to the second heart diasease dataset which has large number of rows-319796 and columns-18 then we have evaluating which classification algorithm best perform in this dataset with how much percentage of accuracy?.
Similarly,Table  \ref{tab:data5} shows that theSVM(Kernel =RBF) achieved the highest performance for the second dataset with accuracy, recall, and precision, which are 91.38$\%$ 100$\%$  and  91$\%$ respectively.The worst performance achieved by Decision Tree with accuracy, recall, and precision, which are 87.57$\%$ 93$\%$  and  93$\%$. Table  \ref{tab:data5} has constructed  as decresing order of accuracy.



\begin{table}[H]
\centering
\caption{ Performance of the same model which has applied to the 2nd dataset}
\vspace{0.3cm}
\def\arraystretch{0.9}
\begin{tabular}{|c|c|c|c|}
\hline
Techniques      &    Accuracy(in $\%$) &  Recall(in $\%$)           &   Precision(in $\%$)  \\ \hline
SVM (RBF)	     &     91.38        &  100			&    91   \\ 
Logistic regression &    91.34		&   100         &      91\\ 
AdaBosst          &      91.31      &   99           &      92\\    
XGBoost          &       91.29      &  99          &       92\\
Gradient boost    &     91.28       &  99           &      92 \\ 
Random forest   &     90.43			&  98		   &    92\\
K-NN 		  &         90.39       &   98		   &     92\\
Decision tree  &       87.57			&  93		   &     93\\
   \hline

\end{tabular}
\label{tab:data5}
\end{table}
\underline{\textbf{The obtained results are also illustrated in the figures given below:}}\\
Figure \ref{fig:fig1} shows the performance of the different algorithms. In this figure, we see that the Random forest predicts whether the person has heart disease or not with 100$\%$ accuracy, Decision Tree and XGBoost were also predicting a better but slightly less percentage of accuracy in comparison to the Random Forest. In the first heart disease dataset, Naive Bayes gives the worst result with an accuracy of 80$\%$.
\vspace{-0.5cm}
\begin{figure}[H]
\centering
\includegraphics[scale=0.79]{test}
\caption{Accuracy comparision of different ML algorithms }
\label{fig:fig1}
\end{figure}

Similarly, Figure  \ref{fig:fiig22} shows the same model performance to the second dataset. Here we see that Kernel(RBF) SVM predicts well in comparison to the other classification algorithms. Here, except for Decision Tree and Naive Bayes, approximately all the algorithms perform quite well, only the 0.000-0.003$\%$ minor variations in the accuracy had shown. Decision Tree gives the worst accuracy which is 86.43$\%$, but Kernel SVM predicts 91.38$\%$.\\
\begin{figure}[H]
\centering
\includegraphics[scale=1]{test_2}
\caption{Accuracy comparision of same ML algorithms to the second dataset }
\label{fig:fiig22}
\end{figure}

Fifure  \ref{fig:fig3} and Figure  \ref{fig:fig4}  Present the mixed bar plots of the Accuracy and F1 Score of the different classification algorithms. The greatest result of the first dataset was \textbf{Random forest} with 100$\%$ accuracy and  100$\%$ F1 score. and for the second dataset \textbf{kernel SVM}  with 91.38$\%$ accuracy and 95$\%$ F1 score.  In Figure 3(1st dataset) we see that F1 score and accuracy have not much variation. But In figure  \ref{fig:fig4} (2nd dataset) F1 score and accuracy of all the algorithms were approximately similar except  DT and Naive Bayes.
\begin{figure}[H]
\centering
\includegraphics[scale=1]{test_4}
\caption{Accuracy vs F1 score for 1st HDD }
\label{fig:fig3}
\end{figure}
\vspace{-0.6cm}
\begin{figure}[H]
\centering
\includegraphics[scale=1]{test_3}
\caption{Accuracy vs F1 score for 2nd HDD  }
\label{fig:fig4}
\end{figure}

\subsection{\underline{Conclusions}}
We apply different classification algorithms and some statistcal tools to predict heart disease in this work. Some Ensemble methos (Boosting) with dimension reduction techniques (PCA) were used which help to reduce the time laps covered by different algorithms and predict the heart disease with best accuracy as well. In this work we also compare the different accuracy and F1 score between ensemble methods(GB,AB,XGBoost) and six classifiers(LG,K-NN,SVM(RBF),DT,RF,NB) which were applied after the PCA.\\
The experimental results showed that the \textbf{Random Forest} in first Heart Disease Dataset(HDD) and \textbf{kernel(RBF)SVM} in second Heart Disease Dataset had achieved the best performance
\subsection{\underline{Availability of the Data}}
\begin{enumerate}
\item The first heart disease dataset are available at  \url {https://www.kaggle.com/johnsmith88/heart-disease-dataset}
\item The second heart disease dataset are available at \url {https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease}

\end{enumerate}
\begin{thebibliography}{}
\bibitem{abc}	
 Arabasadi, Z., Alizadehsani, R., Roshanzamir, M., Moosaei, H. and Yarifard, A. A. Computer aided decision making for heart disease 
detection using hybrid neural network-Genetic algorithm. \textit{Comput. Methods Programs Biomed. 141,} 19–26 (2017).
\bibitem{abc1}
Xiao-Yan Gao,Abdelmegeid Amin Ali, Hassan Shaban Hassan, and Eman M. Anwar. Academic Editor: Ahmed Mostafa Khalil. \textit{Improving the Accuracy for Analyzing Heart Diseases Prediction Based on the Ensemble Method}.19 December 2020
\bibitem{abc2}
Anna Karen Garate-Escamila  , Amir Hajjam El Hassani , Emmanuel Andres\textit{Classification models for heart disease prediction using feature selection and PCA}8 January 2020
\bibitem{abc3}
Yar Muhammad, MuhammadTahir
, Maqsood Hayat1 and  KilTo Chong \textit{Early and accurate detection 
and diagnosis of heart disease 
using intelligent computational 
model}
\bibitem{abc4}
Armin Yazdani, Kasturi Dewi Varathan2 , Yin Kia Chiam
, Asad Waqar Malik and Wan Azman Wan Ahmad \textit{A novel approach for heart disease 
prediction using strength scores with signifcant 
predictors}
\bibitem{abc5}
Khaled Mohamad Almustafa. \textit{Prediction of heart disease and classifiers’
sensitivity analysis}
\bibitem{abc6}
Machine learning A-Z : Hands-On python and R in Data Science \textit{UDEMY}
\bibitem{abc7}
Cardiovascular diseases (CVDs). Retrieved from \url{http://www.who.int/cardiovascular_diseases/en/}
\bibitem{abc8}
Global Burden of Disease Study.  Retrived from \url{https://en.wikipedia.org/wiki/Global_Burden_of_Disease_Study}
\end{thebibliography}





\end{document}




